{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/stas/fastdata/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, GRU, Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import nltk\n",
    "from numba import jit\n",
    "from gensim.models import FastText\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "EMBEDDING_FILE = '../data/crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('../data/train.csv.zip')\n",
    "train[\"comment_text\"] = train.comment_text.apply(lambda x: x[:800])\n",
    "test = pd.read_csv('../data/test.csv.zip')\n",
    "test[\"comment_text\"] = test.comment_text.apply(lambda x: x[:800])\n",
    "submission = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lawer(sen):\n",
    "    count = 0\n",
    "    for i in sen:\n",
    "        if i.isupper():\n",
    "            count += 1\n",
    "    if len(sen) == 0: \n",
    "        return sen\n",
    "    if count/len(sen) > 0.2: \n",
    "        sen.append(\"gronker\")\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stamer(sen):\n",
    "    return list(map(st.stem,sen))\n",
    "st = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_sen(sen):\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in sen]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(sen):\n",
    "    return text.text_to_word_sequence(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:02<00:00, 58584.11it/s]\n",
      "100%|██████████| 153164/153164 [00:02<00:00, 51740.88it/s]\n",
      "100%|██████████| 159571/159571 [00:03<00:00, 45957.26it/s]\n",
      "100%|██████████| 153164/153164 [00:03<00:00, 44508.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 1.86 s, total: 13 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tr = Parallel(n_jobs=16)(delayed(token)(x) for x in tqdm(X_train))\n",
    "tes = Parallel(n_jobs=16)(delayed(token)(x) for x in tqdm(X_test))\n",
    "#gro_tr = Parallel(n_jobs=16)(delayed(lawer)(x) for x in tqdm(tr))\n",
    "#gro_tes = Parallel(n_jobs=16)(delayed(lawer)(x) for x in tqdm(tes))\n",
    "#lem_tr = Parallel(n_jobs=16)(delayed(stamer)(x) for x in tqdm(gro_tr))\n",
    "#lem_tes = Parallel(n_jobs=16)(delayed(stamer)(x) for x in tqdm(gro_tes))\n",
    "keras_ready_tr = Parallel(n_jobs=16)(delayed(token_to_sen)(x) for x in tqdm(tr))\n",
    "keras_ready_tes = Parallel(n_jobs=16)(delayed(token_to_sen)(x) for x in tqdm(tes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load(\"../models/FastText/Fasttest_alpha_0.05_5iter.gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_vec = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_features = 60000\n",
    "maxlen = 800\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features, lower=False)\n",
    "tokenizer.fit_on_texts(keras_ready_tr+keras_ready_tes)\n",
    "X_train = tokenizer.texts_to_sequences(keras_ready_tr)\n",
    "X_test = tokenizer.texts_to_sequences(keras_ready_tes)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embeddings_matrix_self = np.zeros((nb_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = fast_vec.get(word)\n",
    "    if embedding_vector is not None: embeddings_matrix_self[i] = embedding_vector\n",
    "        \n",
    "del(model)\n",
    "del(fast_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "CPU times: user 1min 44s, sys: 2.69 s, total: 1min 46s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index_crawl = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "print(\"ok\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix_crawl = np.zeros((nb_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index_crawl.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix_crawl[i] = embedding_vector\n",
    "        \n",
    "del(embeddings_index_crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''embeddings_index_glove = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(\"../data/glove.840B.300d.txt\"))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embeddings_matrix_glove = np.zeros((nb_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index_glove.get(word)\n",
    "    if embedding_vector is not None: embeddings_index_glove[i] = embedding_vector\n",
    "        \n",
    "del(embeddings_index_glove)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "\n",
    "def get_model(unit=80):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    crawl = Embedding(max_features, embed_size, weights=[embedding_matrix_crawl],trainable = False)(sequence_input)\n",
    "    crawl = SpatialDropout1D(0.2)(crawl)\n",
    "    crawl = Bidirectional(GRU(unit, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(crawl)\n",
    "    crawl = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(crawl)\n",
    "    avg_pool_crawl = GlobalAveragePooling1D()(crawl)\n",
    "    max_pool_crawl = GlobalMaxPooling1D()(crawl)\n",
    "    \n",
    "    self = Embedding(max_features, embed_size, weights=[embeddings_matrix_self],trainable = False)(sequence_input)\n",
    "    self = SpatialDropout1D(0.2)(self)\n",
    "    self = Bidirectional(GRU(unit, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(self)\n",
    "    self = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(self)\n",
    "    avg_pool_self = GlobalAveragePooling1D()(self)\n",
    "    max_pool_self = GlobalMaxPooling1D()(self)\n",
    "    \n",
    "    #glove = Embedding(max_features, embed_size, weights=[embeddings_matrix_glove],trainable = False)(sequence_input)\n",
    "    #glove = SpatialDropout1D(0.2)(glove)\n",
    "    #glove = Bidirectional(GRU(unit, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(glove)\n",
    "    #glove = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(glove)\n",
    "    #avg_pool_glove = GlobalAveragePooling1D()(glove)\n",
    "    #max_pool_glove = GlobalMaxPooling1D()(glove)\n",
    "    \n",
    "    x = concatenate([avg_pool_self, avg_pool_crawl, max_pool_crawl, max_pool_self]) \n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = Dropout(0.1)(x)\n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 1859s 12ms/step - loss: 0.0525 - acc: 0.9811 - val_loss: 0.0453 - val_acc: 0.9821\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.986601 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 1862s 12ms/step - loss: 0.0429 - acc: 0.9836 - val_loss: 0.0426 - val_acc: 0.9834\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988228 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 1935s 13ms/step - loss: 0.0403 - acc: 0.9844 - val_loss: 0.0418 - val_acc: 0.9836\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.989208 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 1882s 12ms/step - loss: 0.0387 - acc: 0.9850 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.989689 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 1875s 12ms/step - loss: 0.0370 - acc: 0.9855 - val_loss: 0.0402 - val_acc: 0.9838\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.990075 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 1873s 12ms/step - loss: 0.0356 - acc: 0.9860 - val_loss: 0.0417 - val_acc: 0.9836\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.989659 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 1872s 12ms/step - loss: 0.0343 - acc: 0.9866 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.989849 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 300\n",
    "epochs = 20\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "for i in range(1,epochs+1):\n",
    "    hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val),\n",
    "                     callbacks=[RocAuc], verbose=1)\n",
    "    model.save(f'../models/Polled_GRU_conv_double_way/{i}epoch.h5')\n",
    "\n",
    "    y_pred = model.predict(x_test, batch_size=batch_size)\n",
    "    submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "    submission.to_csv(f'../submits/Polled_GRU_conv_double_way/{i}epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
