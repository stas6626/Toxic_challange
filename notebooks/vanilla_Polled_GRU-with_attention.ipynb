{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/stas/fastdata/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Dropout\n",
    "from keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, GRU, Conv1D, LSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "EMBEDDING_FILE = '../data/crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('../data/train.csv.zip')\n",
    "test = pd.read_csv('../data/test.csv.zip')\n",
    "submission = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 60000\n",
    "maxlen = 400\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, 400))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(400, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    print(inputs)\n",
    "    print(a_probs)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"spatial_dropout1d_8/cond/Merge:0\", shape=(?, 400, 300), dtype=float32)\n",
      "Tensor(\"attention_vec_4/transpose:0\", shape=(?, 400, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "\n",
    "def get_model(units=80):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = attention_3d_block(x)\n",
    "    x = Bidirectional(CuDNNGRU(units, return_sequences=True))(x)\n",
    "    x= Flatten()(x)\n",
    "    #avg_pool = GlobalAveragePooling1D()(x)\n",
    "    #max_pool = GlobalMaxPooling1D()(x)\n",
    "    #conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    ad = Adam(lr=0.005)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer= ad,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 90s 593us/step - loss: 0.0981 - acc: 0.9707 - val_loss: 0.0668 - val_acc: 0.9770\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.965598 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 87s 576us/step - loss: 0.0500 - acc: 0.9821 - val_loss: 0.0554 - val_acc: 0.9797\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.978970 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 86s 565us/step - loss: 0.0357 - acc: 0.9861 - val_loss: 0.0551 - val_acc: 0.9808\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.978256 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 86s 567us/step - loss: 0.0250 - acc: 0.9904 - val_loss: 0.0581 - val_acc: 0.9806\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.977298 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 86s 568us/step - loss: 0.0169 - acc: 0.9937 - val_loss: 0.0742 - val_acc: 0.9801\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.978552 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 87s 571us/step - loss: 0.0110 - acc: 0.9961 - val_loss: 0.0744 - val_acc: 0.9796\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.974941 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 86s 571us/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0819 - val_acc: 0.9800\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.976094 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      " 49700/151592 [========>.....................] - ETA: 57s - loss: 0.0054 - acc: 0.9984"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 700\n",
    "epochs = 20\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "for i in range(1,epochs+1):\n",
    "    hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val),\n",
    "                     callbacks=[RocAuc], verbose=1)\n",
    "    model.save(f'../models/vanilla_Polled_gru_with_attention/{i}epoch.h5')\n",
    "\n",
    "    y_pred = model.predict(x_test, batch_size=1024)\n",
    "    submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "    submission.to_csv(f'../submits/vanilla_Polled_gru_with_attention/{i}epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/20\n",
      "143613/143613 [==============================] - 59s 411us/step - loss: 0.0716 - acc: 0.9763 - val_loss: 0.0445 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04447, saving model to ../models/vanilla_gru_cv_10/fold_1.best.hdf5\n",
      "Epoch 2/20\n",
      "143613/143613 [==============================] - 59s 410us/step - loss: 0.0386 - acc: 0.9851 - val_loss: 0.0413 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04447 to 0.04133, saving model to ../models/vanilla_gru_cv_10/fold_1.best.hdf5\n",
      "Epoch 3/20\n",
      "143613/143613 [==============================] - 57s 397us/step - loss: 0.0307 - acc: 0.9878 - val_loss: 0.0431 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/20\n",
      "143614/143614 [==============================] - 57s 398us/step - loss: 0.0748 - acc: 0.9745 - val_loss: 0.0453 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04534, saving model to ../models/vanilla_gru_cv_10/fold_2.best.hdf5\n",
      "Epoch 2/20\n",
      "143614/143614 [==============================] - 57s 398us/step - loss: 0.0380 - acc: 0.9852 - val_loss: 0.0424 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04534 to 0.04236, saving model to ../models/vanilla_gru_cv_10/fold_2.best.hdf5\n",
      "Epoch 3/20\n",
      "143614/143614 [==============================] - 57s 396us/step - loss: 0.0303 - acc: 0.9880 - val_loss: 0.0450 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/20\n",
      "143614/143614 [==============================] - 57s 399us/step - loss: 0.0739 - acc: 0.9746 - val_loss: 0.0424 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04242, saving model to ../models/vanilla_gru_cv_10/fold_3.best.hdf5\n",
      "Epoch 2/20\n",
      "143614/143614 [==============================] - 57s 399us/step - loss: 0.0384 - acc: 0.9849 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04242 to 0.04032, saving model to ../models/vanilla_gru_cv_10/fold_3.best.hdf5\n",
      "Epoch 3/20\n",
      "143614/143614 [==============================] - 57s 399us/step - loss: 0.0304 - acc: 0.9879 - val_loss: 0.0434 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/20\n",
      "143614/143614 [==============================] - 58s 405us/step - loss: 0.0740 - acc: 0.9740 - val_loss: 0.0440 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04400, saving model to ../models/vanilla_gru_cv_10/fold_4.best.hdf5\n",
      "Epoch 2/20\n",
      "143614/143614 [==============================] - 60s 415us/step - loss: 0.0384 - acc: 0.9850 - val_loss: 0.0419 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04400 to 0.04191, saving model to ../models/vanilla_gru_cv_10/fold_4.best.hdf5\n",
      "Epoch 3/20\n",
      "143614/143614 [==============================] - 59s 413us/step - loss: 0.0308 - acc: 0.9878 - val_loss: 0.0435 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/20\n",
      "143614/143614 [==============================] - 61s 427us/step - loss: 0.0741 - acc: 0.9755 - val_loss: 0.0430 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04299, saving model to ../models/vanilla_gru_cv_10/fold_5.best.hdf5\n",
      "Epoch 2/20\n",
      "143614/143614 [==============================] - 60s 418us/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0402 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04299 to 0.04018, saving model to ../models/vanilla_gru_cv_10/fold_5.best.hdf5\n",
      "Epoch 3/20\n",
      "143614/143614 [==============================] - 60s 415us/step - loss: 0.0312 - acc: 0.9875 - val_loss: 0.0429 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/20\n",
      "143614/143614 [==============================] - 61s 423us/step - loss: 0.0754 - acc: 0.9733 - val_loss: 0.0443 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04433, saving model to ../models/vanilla_gru_cv_10/fold_6.best.hdf5\n",
      "Epoch 2/20\n",
      "143614/143614 [==============================] - 60s 417us/step - loss: 0.0381 - acc: 0.9851 - val_loss: 0.0420 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04433 to 0.04198, saving model to ../models/vanilla_gru_cv_10/fold_6.best.hdf5\n",
      "Epoch 3/20\n",
      "143614/143614 [==============================] - 61s 422us/step - loss: 0.0305 - acc: 0.9878 - val_loss: 0.0441 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/20\n",
      "143614/143614 [==============================] - 60s 418us/step - loss: 0.0714 - acc: 0.9771 - val_loss: 0.0432 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04316, saving model to ../models/vanilla_gru_cv_10/fold_7.best.hdf5\n",
      "Epoch 2/20\n",
      "143614/143614 [==============================] - 59s 414us/step - loss: 0.0381 - acc: 0.9853 - val_loss: 0.0411 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04316 to 0.04110, saving model to ../models/vanilla_gru_cv_10/fold_7.best.hdf5\n",
      "Epoch 3/20\n",
      "143614/143614 [==============================] - 59s 414us/step - loss: 0.0306 - acc: 0.9879 - val_loss: 0.0429 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/20\n",
      "143614/143614 [==============================] - 60s 421us/step - loss: 0.0739 - acc: 0.9744 - val_loss: 0.0435 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04352, saving model to ../models/vanilla_gru_cv_10/fold_8.best.hdf5\n",
      "Epoch 2/20\n",
      "143614/143614 [==============================] - 60s 418us/step - loss: 0.0380 - acc: 0.9852 - val_loss: 0.0406 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04352 to 0.04065, saving model to ../models/vanilla_gru_cv_10/fold_8.best.hdf5\n",
      "Epoch 3/20\n",
      "143614/143614 [==============================] - 62s 429us/step - loss: 0.0309 - acc: 0.9877 - val_loss: 0.0444 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/20\n",
      "143614/143614 [==============================] - 61s 425us/step - loss: 0.0734 - acc: 0.9748 - val_loss: 0.0454 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04542, saving model to ../models/vanilla_gru_cv_10/fold_9.best.hdf5\n",
      "Epoch 2/20\n",
      "143614/143614 [==============================] - 61s 424us/step - loss: 0.0385 - acc: 0.9850 - val_loss: 0.0432 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04542 to 0.04316, saving model to ../models/vanilla_gru_cv_10/fold_9.best.hdf5\n",
      "Epoch 3/20\n",
      "143614/143614 [==============================] - 60s 416us/step - loss: 0.0310 - acc: 0.9877 - val_loss: 0.0434 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/20\n",
      "143614/143614 [==============================] - 61s 426us/step - loss: 0.0716 - acc: 0.9771 - val_loss: 0.0432 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04325, saving model to ../models/vanilla_gru_cv_10/fold_10.best.hdf5\n",
      "Epoch 2/20\n",
      "143614/143614 [==============================] - 61s 422us/step - loss: 0.0384 - acc: 0.9851 - val_loss: 0.0415 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04325 to 0.04146, saving model to ../models/vanilla_gru_cv_10/fold_10.best.hdf5\n",
      "Epoch 3/20\n",
      "143614/143614 [==============================] - 59s 408us/step - loss: 0.0307 - acc: 0.9877 - val_loss: 0.0442 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "CPU times: user 24min 48s, sys: 4min 25s, total: 29min 14s\n",
      "Wall time: 30min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gru_for_stack = pd.DataFrame.from_dict({'id': train['id'],\n",
    "                                        \"toxic\":0,\n",
    "                                        \"severe_toxic\":0,\n",
    "                                        \"obscene\":0,\n",
    "                                        \"threat\":0,\n",
    "                                        \"insult\":0,\n",
    "                                        \"identity_hate\":0})\n",
    "\n",
    "batch_size = 800\n",
    "epochs = 20\n",
    "cv = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "i = 0\n",
    "\n",
    "for train_index, test_index in cv.split(x_train, y_train):\n",
    "    X_tra, X_test = x_train[train_index], x_train[test_index]\n",
    "    y_tra, y_test = y_train[train_index], y_train[test_index]\n",
    "    i += 1\n",
    "    \n",
    "    model = get_model()\n",
    "    file_path=f\"../models/vanilla_gru_cv_10/fold_{i}.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\")\n",
    "    callbacks_list = [checkpoint, early] #early\n",
    "    #exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "    #steps = int(len(X_tra)/batch_size) * epochs\n",
    "    #lr_init, lr_fin = 0.001, 0.0005\n",
    "    #lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "    #K.set_value(model.optimizer.lr, lr_init)\n",
    "    #K.set_value(model.optimizer.decay, lr_decay)\n",
    "\n",
    "    hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, callbacks=callbacks_list, validation_data=(X_test, y_test))\n",
    "\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    y_pred = model.predict(X_test, batch_size=1024)\n",
    "    gru_for_stack.loc[test_index,[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "gru_for_stack.to_csv('../submits/vanilla_gru_cv_10/fold_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.001095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.004207</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.034808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  identity_hate    insult   obscene  severe_toxic  \\\n",
       "0  0000997932d777bf       0.000027  0.000102  0.000097      0.000015   \n",
       "1  000103f0d9cfb60f       0.000062  0.000082  0.000109      0.000019   \n",
       "2  000113f07ec002fd       0.000060  0.000136  0.000473      0.000076   \n",
       "3  0001b41b1c6bb37e       0.000018  0.000059  0.000094      0.000022   \n",
       "4  0001d958c54c6e35       0.000400  0.004207  0.001973      0.000241   \n",
       "\n",
       "     threat     toxic  \n",
       "0  0.000037  0.000405  \n",
       "1  0.000015  0.000741  \n",
       "2  0.000090  0.001095  \n",
       "3  0.000018  0.000213  \n",
       "4  0.000584  0.034808  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_for_stack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../models/vanilla_gru_cv_10/fold_1.best.hdf5\")\n",
    "y_pred = model.predict(x_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 20.7 s, total: 2min 20s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(2,11):\n",
    "    model.load_weights(f\"../models/vanilla_gru_cv_10/fold_{i}.best.hdf5\")\n",
    "    y_pred *= model.predict(x_test, batch_size=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred ** 0.1\n",
    "submission.to_csv('../submits/vanilla_gru_cv_10/submitiongeomaverage.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1)\n",
    "model.save('../models/vanilla_Polled_GRU_onlypooling/2epoch.h5')\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('../submits/vanilla_Polled_GRU_onlypooling/2epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 234s 2ms/step - loss: 0.0353 - acc: 0.9860 - val_loss: 0.0416 - val_acc: 0.9842\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.989550 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1)\n",
    "model.save('../models/vanilla_Polled_GRU_onlypooling/2epoch.h5')\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('../submits/vanilla_Polled_GRU_onlypooling/2epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 236s 2ms/step - loss: 0.0291 - acc: 0.9884 - val_loss: 0.0448 - val_acc: 0.9835\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988765 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1)\n",
    "model.save('../models/vanilla_Polled_GRU_onlypooling/3epoch.h5')\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('../submits/vanilla_Polled_GRU_onlypooling/3epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151592/151592 [==============================] - 236s 2ms/step - loss: 0.0229 - acc: 0.9910 - val_loss: 0.0493 - val_acc: 0.9827\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988463 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1)\n",
    "model.save('../models/vanilla_Polled_GRU_onlypooling/4epoch.h5')\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('../submits/vanilla_Polled_GRU_onlypooling/4epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
